{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozFGpw_avGF2",
        "outputId": "5e3a3c0e-ba7f-48b1-898d-71cc0c9e8bd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m2988/2988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 13ms/step - loss: 2.7385 - sdoc_output_accuracy: 0.5866 - tdoc_output_accuracy: 0.6835 - val_loss: 1.3857 - val_sdoc_output_accuracy: 0.7761 - val_tdoc_output_accuracy: 0.8457\n",
            "Epoch 2/10\n",
            "\u001b[1m2988/2988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 12ms/step - loss: 1.3454 - sdoc_output_accuracy: 0.7839 - tdoc_output_accuracy: 0.8533 - val_loss: 1.2215 - val_sdoc_output_accuracy: 0.7999 - val_tdoc_output_accuracy: 0.8607\n",
            "Epoch 3/10\n",
            "\u001b[1m2988/2988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - loss: 1.1320 - sdoc_output_accuracy: 0.8131 - tdoc_output_accuracy: 0.8735 - val_loss: 1.1786 - val_sdoc_output_accuracy: 0.8079 - val_tdoc_output_accuracy: 0.8634\n",
            "Epoch 4/10\n",
            "\u001b[1m2988/2988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - loss: 1.0214 - sdoc_output_accuracy: 0.8292 - tdoc_output_accuracy: 0.8861 - val_loss: 1.1670 - val_sdoc_output_accuracy: 0.8095 - val_tdoc_output_accuracy: 0.8637\n",
            "Epoch 5/10\n",
            "\u001b[1m2988/2988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - loss: 0.9428 - sdoc_output_accuracy: 0.8417 - tdoc_output_accuracy: 0.8934 - val_loss: 1.1783 - val_sdoc_output_accuracy: 0.8120 - val_tdoc_output_accuracy: 0.8646\n",
            "Epoch 6/10\n",
            "\u001b[1m2988/2988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 13ms/step - loss: 0.8744 - sdoc_output_accuracy: 0.8508 - tdoc_output_accuracy: 0.9022 - val_loss: 1.1984 - val_sdoc_output_accuracy: 0.8125 - val_tdoc_output_accuracy: 0.8657\n",
            "Epoch 7/10\n",
            "\u001b[1m2988/2988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 13ms/step - loss: 0.8217 - sdoc_output_accuracy: 0.8576 - tdoc_output_accuracy: 0.9078 - val_loss: 1.2093 - val_sdoc_output_accuracy: 0.8120 - val_tdoc_output_accuracy: 0.8641\n",
            "Epoch 8/10\n",
            "\u001b[1m2988/2988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 13ms/step - loss: 0.7650 - sdoc_output_accuracy: 0.8651 - tdoc_output_accuracy: 0.9170 - val_loss: 1.2419 - val_sdoc_output_accuracy: 0.8128 - val_tdoc_output_accuracy: 0.8658\n",
            "Epoch 9/10\n",
            "\u001b[1m2988/2988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 13ms/step - loss: 0.7353 - sdoc_output_accuracy: 0.8693 - tdoc_output_accuracy: 0.9186 - val_loss: 1.2613 - val_sdoc_output_accuracy: 0.8163 - val_tdoc_output_accuracy: 0.8677\n",
            "Epoch 10/10\n",
            "\u001b[1m2988/2988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 12ms/step - loss: 0.6946 - sdoc_output_accuracy: 0.8761 - tdoc_output_accuracy: 0.9241 - val_loss: 1.2975 - val_sdoc_output_accuracy: 0.8148 - val_tdoc_output_accuracy: 0.8671\n",
            "\u001b[1m747/747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 1.2784 - sdoc_output_accuracy: 0.8193 - tdoc_output_accuracy: 0.8702\n",
            "Loss Total: 1.2974509000778198\n",
            "Accuracy para NOMBRE_TDOC: 0.8148380517959595\n",
            "Accuracy para NOMBRE_SDOC: 0.8670600056648254\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
            "Predicción NOMBRE_TDOC: Recursos\n",
            "Predicción NOMBRE_SDOC: Declaración IIVTNU\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils import class_weight\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Cargar el dataset\n",
        "data = pd.read_csv('DS2.csv', delimiter=';', encoding='utf-8')\n",
        "\n",
        "# Asegurarse de que todos los valores en TEXTO_SOLICITUD_PREL sean cadenas de texto\n",
        "data['TEXTO_SOLICITUD_PREL'] = data['TEXTO_SOLICITUD_PREL'].astype(str)\n",
        "\n",
        "# Opcional: Manejar valores nulos, si existen\n",
        "data['TEXTO_SOLICITUD_PREL'].fillna('texto desconocido', inplace=True)\n",
        "\n",
        "# Preprocesamiento de textos\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(data['TEXTO_SOLICITUD_PREL'])\n",
        "X = tokenizer.texts_to_sequences(data['TEXTO_SOLICITUD_PREL'])\n",
        "X = pad_sequences(X, maxlen=200)\n",
        "\n",
        "# Preprocesar las variables categóricas\n",
        "label_encoder_tdoc = LabelEncoder()\n",
        "label_encoder_sdoc = LabelEncoder()\n",
        "\n",
        "y_tdoc = label_encoder_tdoc.fit_transform(data['NOMBRE_TDOC'])\n",
        "y_sdoc = label_encoder_sdoc.fit_transform(data['NOMBRE_SDOC'])\n",
        "\n",
        "y_tdoc = to_categorical(y_tdoc)\n",
        "y_sdoc = to_categorical(y_sdoc)\n",
        "\n",
        "# Dividir el dataset\n",
        "X_train, X_test, y_tdoc_train, y_tdoc_test, y_sdoc_train, y_sdoc_test = train_test_split(X, y_tdoc, y_sdoc, test_size=0.2, random_state=42)\n",
        "\n",
        "# Crear la red neuronal\n",
        "input_text = Input(shape=(200,))\n",
        "embedding = Embedding(input_dim=5000, output_dim=128)(input_text)\n",
        "lstm = LSTM(128, return_sequences=False)(embedding)\n",
        "dropout = Dropout(0.5)(lstm)\n",
        "\n",
        "# Salida para NOMBRE_TDOC\n",
        "output_tdoc = Dense(y_tdoc.shape[1], activation='softmax', name='tdoc_output')(dropout)\n",
        "\n",
        "# Salida para NOMBRE_SDOC\n",
        "output_sdoc = Dense(y_sdoc.shape[1], activation='softmax', name='sdoc_output')(dropout)\n",
        "\n",
        "# Modelo\n",
        "model = Model(inputs=input_text, outputs=[output_tdoc, output_sdoc])\n",
        "model.compile(optimizer='adam',\n",
        "              loss={'tdoc_output': 'categorical_crossentropy', 'sdoc_output': 'categorical_crossentropy'},\n",
        "              metrics={'tdoc_output': 'accuracy', 'sdoc_output': 'accuracy'},\n",
        "              loss_weights={'tdoc_output': 1.0, 'sdoc_output': 1.0})\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(X_train, [y_tdoc_train, y_sdoc_train], epochs=10, batch_size=32, validation_data=(X_test, [y_tdoc_test, y_sdoc_test]))\n",
        "\n",
        "# Evaluar el modelo\n",
        "loss, tdoc_acc, sdoc_acc = model.evaluate(X_test, [y_tdoc_test, y_sdoc_test])\n",
        "print(f\"Loss Total: {loss}\")\n",
        "print(f\"Accuracy para NOMBRE_TDOC: {tdoc_acc}\")\n",
        "print(f\"Accuracy para NOMBRE_SDOC: {sdoc_acc}\")\n",
        "\n",
        "# Predicción para un nuevo texto\n",
        "def predict(text):\n",
        "    seq = tokenizer.texts_to_sequences([text])\n",
        "    padded_seq = pad_sequences(seq, maxlen=200)\n",
        "    pred_tdoc, pred_sdoc = model.predict(padded_seq)\n",
        "    tdoc_label = label_encoder_tdoc.inverse_transform([np.argmax(pred_tdoc)])\n",
        "    sdoc_label = label_encoder_sdoc.inverse_transform([np.argmax(pred_sdoc)])\n",
        "    return tdoc_label[0], sdoc_label[0]\n",
        "\n",
        "# Ejemplo de uso\n",
        "new_text = \"Introduce aquí el texto que deseas clasificar\"\n",
        "predicted_tdoc, predicted_sdoc = predict(new_text)\n",
        "print(f\"Predicción NOMBRE_TDOC: {predicted_tdoc}\")\n",
        "print(f\"Predicción NOMBRE_SDOC: {predicted_sdoc}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de uso\n",
        "new_text = \"Solicitar pagar menos para coche minusvalio\"\n",
        "predicted_tdoc, predicted_sdoc = predict(new_text)\n",
        "print(f\"Predicción NOMBRE_TDOC: {predicted_tdoc}\")\n",
        "print(f\"Predicción NOMBRE_SDOC: {predicted_sdoc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thBnDrxO4ASV",
        "outputId": "b2b83152-557a-4fc5-fdd5-e0f88c1f94c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "Predicción NOMBRE_TDOC: Solicitudes\n",
            "Predicción NOMBRE_SDOC: Otras solicitudes tributarias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar el modelo completo\n",
        "model.save('modelo_clasificacion_01.h5')\n",
        "\n",
        "# Guardar los tokenizadores y label encoders (puedes utilizar pickle)\n",
        "import pickle\n",
        "\n",
        "# Guardar el tokenizer\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Guardar los label encoders\n",
        "with open('label_encoder_tdoc.pickle', 'wb') as handle:\n",
        "    pickle.dump(label_encoder_tdoc, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('label_encoder_sdoc.pickle', 'wb') as handle:\n",
        "    pickle.dump(label_encoder_sdoc, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_3Ou6lVUvpe",
        "outputId": "867e24bc-6893-4422-dba5-68b18e2cdffe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    }
  ]
}